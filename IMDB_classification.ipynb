{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from embeddings import get_embedding_matrix, sentence_to_indexes\n",
    "from emb_path import glove_6B_300d_path, lexvec_7B_300d_path, glove_6B_50d_path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gem import SentenceEmbedder\n",
    "from scipy.stats import pearsonr\n",
    "from utils import read_sts\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_PATH = './data/imdb'\n",
    "\n",
    "IMDB_TRAIN = os.path.join(IMDB_PATH, 'train')\n",
    "IMDB_TRAIN_POS = os.path.join(IMDB_TRAIN, 'pos')\n",
    "IMDB_TRAIN_NEG = os.path.join(IMDB_TRAIN, 'neg')\n",
    "\n",
    "IMDB_TEST = os.path.join(IMDB_PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_samples = [os.path.join(IMDB_TRAIN_POS, s) for s in os.listdir(IMDB_TRAIN_POS)]\n",
    "train_neg_samples = [os.path.join(IMDB_TRAIN_NEG, s) for s in os.listdir(IMDB_TRAIN_NEG)]\n",
    "\n",
    "test_samples = [os.path.join(IMDB_TEST, s) for s in os.listdir(IMDB_TEST)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 12501 11001\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pos_samples), len(train_neg_samples), len(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process every review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e656ca1ab5c481993ba41783f9f3660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7865ae4d219f4d4cb4190224f2061f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12501), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_reviews = []\n",
    "neg_reviews = []\n",
    "\n",
    "try:\n",
    "    for review in tqdm(train_pos_samples[:]):\n",
    "        with open(review, 'r', encoding=\"utf-8\") as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            data = data.replace('<br />', ' ')\n",
    "            data = data.replace('.', '')\n",
    "    #         data_sentences = tokenizer.tokenize(data)\n",
    "            pos_reviews += [data]\n",
    "except UnicodeDecodeError:\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    for review in tqdm(train_neg_samples[:]):\n",
    "        with open(review, 'r', encoding=\"utf-8\") as file:\n",
    "            data = file.read().replace('\\n', '')\n",
    "            data = data.replace('<br />', ' ')\n",
    "            data = data.replace('.', '')\n",
    "    #         data_sentences = tokenizer.tokenize(data)\n",
    "            neg_reviews += [data]\n",
    "except UnicodeDecodeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "labels = [1]*len(pos_reviews) + [0]*len(neg_reviews)\n",
    "reviews = pos_reviews + neg_reviews\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy It ran at the same time as some other programs about school life, such as \"Teachers\" My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\" The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled  at  High A classic line: INSPECTOR: I\\'m here to sack one of your teachers STUDENT: Welcome to Bromwell High I expect that many adults of my age think that Bromwell High is far fetched What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embds = {\"Glove\": get_embedding_matrix(glove_6B_300d_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25000it [22:18, 18.67it/s]\n",
      "4767it [24:30,  3.24it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "25000it [2:09:01,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13h 48min 37s, sys: 1d 18h 1min 28s, total: 2d 7h 50min 5s\n",
      "Wall time: 2h 31min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "print('Unigrams:\\n\\n')\n",
    "\n",
    "for e_name, (e, v) in embds.items():\n",
    "    model = SentenceEmbedder(reviews, e, v, False)\n",
    "    result, s = model.gem(sigma_power=3)\n",
    "    embeddings += [result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('imdb_embeddings.npy', embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_embeddings = np.load('imdb_embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imdb = np.append(imdb_embeddings[:10000], imdb_embeddings[12500:22500], axis=0)\n",
    "train_imdb_labels = np.array([1]*10000+[0]*10000)\n",
    "\n",
    "test_imdb = np.append(imdb_embeddings[10000:12500], imdb_embeddings[22500:], axis=0)\n",
    "test_imdb_labels = np.array([1]*2500+[0]*2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit&Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(train_imdb, train_imdb_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
