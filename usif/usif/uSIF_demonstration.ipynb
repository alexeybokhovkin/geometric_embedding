{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.linalg import svd\n",
    "import nltk\n",
    "import sys\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2prob(object):\n",
    "    \"\"\"Map words to their probabilities.\"\"\"\n",
    "    def __init__(self, count_fn):\n",
    "        \"\"\"Initialize a word2prob object.\n",
    "\n",
    "        Args:\n",
    "            count_fn: word count file name (one word per line) \n",
    "        \"\"\"\n",
    "        self.prob = {}\n",
    "        total = 0.0\n",
    "\n",
    "        for line in open(count_fn):\n",
    "            k, v = line.split()\n",
    "            v = int(v)\n",
    "            k = k.lower()\n",
    "\n",
    "            self.prob[k] = v\n",
    "            total += v\n",
    "\n",
    "        self.prob = {k : (self.prob[k] / total) for k in self.prob}\n",
    "        self.min_prob = min(self.prob.values())\n",
    "        self.count = total\n",
    "\n",
    "    def __getitem__(self, w):\n",
    "        return self.prob.get(w.lower(), self.min_prob)\n",
    "\n",
    "    def __contains__(self, w):\n",
    "        return w.lower() in self.prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prob)\n",
    "\n",
    "    def vocab(self):\n",
    "        return iter(self.prob.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec(object):\n",
    "    \"\"\"Map words to their embeddings.\"\"\"\n",
    "    def __init__(self, vector_fn):\n",
    "        \"\"\"Initialize a word2vec object.\n",
    "\n",
    "        Args:\n",
    "            vector_fn: embedding file name (one word per line)\n",
    "        \"\"\"\n",
    "        self.vectors = {}\n",
    "\n",
    "        for line in open(vector_fn, 'r', encoding=\"utf-8\"):\n",
    "            line = line.split()\n",
    "\n",
    "            # skip first line if needed\n",
    "            if len(line) == 2:\n",
    "                continue\n",
    "\n",
    "            word = line[0]\n",
    "            embedding = np.array([float(val) for val in line[1:]])\n",
    "            self.vectors[word] = embedding\n",
    "\n",
    "    def __getitem__(self, w):\n",
    "        return self.vectors[w]\n",
    "\n",
    "    def __contains__(self, w):\n",
    "        return w in self.vectors\n",
    "    \n",
    "    def _print(self):\n",
    "        print(self.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class uSIF(object):\n",
    "    \"\"\"Embed sentences using unsupervised smoothed inverse frequency.\"\"\"\n",
    "    def __init__(self, vec, prob, n=11, m=5):\n",
    "        \"\"\"Initialize a sent2vec object.\n",
    "\n",
    "        Variable names (e.g., alpha, a) all carry over from the paper.\n",
    "\n",
    "        Args:\n",
    "            vec: word2vec object\n",
    "            prob: word2prob object\n",
    "            n: expected random walk length. This is the avg sentence length, which\n",
    "                should be estimated from a large representative sample. For STS\n",
    "                tasks, n ~ 11. n should be a positive integer.\n",
    "            m: number of common discourse vectors (in practice, no more than 5 needed)\n",
    "        \"\"\"\n",
    "        self.vec = vec\n",
    "        self.m = m\n",
    "\n",
    "        if not (isinstance(n, int) and n > 0):\n",
    "            raise TypeError(\"n should be a positive integer\")\n",
    "\n",
    "        vocab_size = float(len(prob))\n",
    "        threshold = 1 - (1 - 1/vocab_size) ** n\n",
    "        alpha = len([ w for w in prob.vocab() if prob[w] > threshold ]) / vocab_size\n",
    "        Z = 0.5 * vocab_size\n",
    "        self.a = (1 - alpha)/(alpha * Z)\n",
    "\n",
    "        self.weight = lambda word: (self.a / (0.5 * self.a + prob[word])) \n",
    "\n",
    "    def _to_vec(self, sentence):\n",
    "        \"\"\"Vectorize a given sentence.\n",
    "\n",
    "        Args:\n",
    "            sentence: a sentence (string) \n",
    "        \"\"\"\n",
    "        # regex for non-punctuation\n",
    "        not_punc = re.compile('.*[A-Za-z0-9].*')\n",
    "\n",
    "        # preprocess a given token\n",
    "        def preprocess(t):\n",
    "            t = t.lower().strip(\"';.:()\").strip('\"')\n",
    "            t = 'not' if t == \"n't\" else t\n",
    "            return t\n",
    "\n",
    "#         tokens = map(preprocess, filter(lambda t: not_punc.match(t), nltk.word_tokenize(sentence)))\n",
    "        tokens = [preprocess(x) for x in filter(lambda t: not_punc.match(t), nltk.word_tokenize(sentence))]\n",
    "#         tokens = reduce(lambda a, b: a + b, [[]] + map(lambda t: re.split(r'[-]', t), tokens))\n",
    "        splitter = lambda t: re.split(r'[-]', t)\n",
    "        tokens = reduce(lambda a, b: a + b, [[]] + [splitter(x) for x in tokens])\n",
    "        \n",
    "        tokens = list(filter(lambda t: t in self.vec, tokens))\n",
    "        \n",
    "        weighter = lambda i, t: self.weight(t) * v_t[i,:]\n",
    "        vectorizer = lambda i, t: self.vec[t]\n",
    "        # if no parseable tokens, return a vector of a's  \n",
    "        if tokens == []:\n",
    "            return np.zeros(300) + self.a\n",
    "        else:\n",
    "#             v_t = np.array(map(lambda i, t: self.vec[t], enumerate(tokens)))\n",
    "            v_t = np.array([vectorizer(i, t) for i, t in enumerate(tokens)])\n",
    "            v_t = v_t * (1.0 / np.linalg.norm(v_t, axis=0))\n",
    "#             v_t = np.array(map(lambda i, t: self.weight(t) * v_t[i,:], enumerate(tokens)))\n",
    "            v_t = np.array([weighter(i, t) for i,t in enumerate(tokens)])\n",
    "            return np.mean(v_t, axis=0) \n",
    "\n",
    "    def embed(self, sentences):\n",
    "        \"\"\"Embed a list of sentences.\n",
    "\n",
    "        Args:\n",
    "            sentences: a list of sentences (strings)\n",
    "        \"\"\"\n",
    "#         vectors = map(self._to_vec, sentences)\n",
    "        vectors = [self._to_vec(sentence) for sentence in sentences]\n",
    "        if self.m == 0:\n",
    "            return vectors\n",
    "\n",
    "        proj = lambda a, b: a.dot(b.transpose()) * b\n",
    "        svd = TruncatedSVD(n_components=self.m, random_state=0).fit(vectors)\n",
    "\n",
    "        projector = lambda v_s: v_s - lambda_i * proj(v_s, pc)\n",
    "        # remove the weighted projections on the common discourse vectors\n",
    "        for i in range(self.m):\n",
    "            lambda_i = (svd.singular_values_[i] ** 2) / (svd.singular_values_ ** 2).sum()\n",
    "            pc = svd.components_[i]\n",
    "#             vectors = map(lambda v_s: v_s - lambda_i * proj(v_s, pc), vectors) \n",
    "            vectors = [projector(v_s) for v_s in vectors]\n",
    "\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_STS(model):\n",
    "    \"\"\"Test the performance on the STS tasks and print out the results.\n",
    "\n",
    "    Expected results:\n",
    "        STS2012: 0.683\n",
    "        STS2013: 0.661\n",
    "        STS2014: 0.784\n",
    "        STS2015: 0.790\n",
    "        SICK2014: 0.735\n",
    "        STSBenchmark: 0.795\n",
    "\n",
    "    Args:\n",
    "        model: a uSIF object\n",
    "    \"\"\" \n",
    "    test_dirs = [\n",
    "        'STS/STS-data/STS2012-gold/',\n",
    "        'STS/STS-data/STS2013-gold/',\n",
    "        'STS/STS-data/STS2014-gold/',\n",
    "        'STS/STS-data/STS2015-gold/',\n",
    "        'STS/SICK-data/',\n",
    "        'STSBenchmark/'\n",
    "    ]\n",
    "\n",
    "    for td in test_dirs:\n",
    "        test_fns = list(filter(lambda fn: '.input.' in fn and fn.endswith('txt'), os.listdir(td)))\n",
    "        scores = []\n",
    "\n",
    "        for fn in test_fns:\n",
    "            sentences = re.split(r'\\t|\\n', open(td + fn, encoding=\"utf-8\").read().strip())\n",
    "            vectors = model.embed(sentences)\n",
    "            y_hat = [1 - cosine(vectors[i], vectors[i+1]) for i in range(0, len(vectors), 2)]\n",
    "#             y = map(float, open(td + fn.replace('input', 'gs')).read().strip().split('\\n'))\n",
    "            y = [float(x) for x in open(td + fn.replace('input', 'gs')).read().strip().split('\\n')]\n",
    "\n",
    "            score = pearsonr(y, y_hat)[0]\n",
    "            scores.append(score)\n",
    "\n",
    "            print(fn, \"\\t\", score)\n",
    "\n",
    "        print(td, np.mean(scores), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paranmt_usif():\n",
    "    \"\"\"Return a uSIF embedding model that used pre-trained ParaNMT word vectors.\"\"\"\n",
    "    prob = word2prob('enwiki_vocab_min200.txt')\n",
    "    vec = word2vec('vectors/czeng.txt')\n",
    "    return uSIF(vec, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_paranmt_usif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS.input.MSRpar.txt \t 0.5832344220178628\n",
      "STS.input.MSRvid.txt \t 0.90059731875375\n",
      "STS.input.SMTeuroparl.txt \t 0.54678977813508\n",
      "STS.input.surprise.OnWN.txt \t 0.7424428233207502\n",
      "STS.input.surprise.SMTnews.txt \t 0.6405004439431515\n",
      "STS/STS-data/STS2012-gold/ 0.6827129572341188 \n",
      "\n",
      "STS.input.FNWN.txt \t 0.5362598491434404\n",
      "STS.input.OnWN.txt \t 0.8751195336553216\n",
      "STS.input.SMT.txt \t 0.4201164598100904\n",
      "STS.input.headlines.txt \t 0.8120590733052622\n",
      "STS/STS-data/STS2013-gold/ 0.6608887289785286 \n",
      "\n",
      "STS.input.OnWN.txt \t 0.8886050369707091\n",
      "STS.input.deft-forum.txt \t 0.585211949036511\n",
      "STS.input.deft-news.txt \t 0.7836139433268715\n",
      "STS.input.headlines.txt \t 0.7908262011798272\n",
      "STS.input.images.txt \t 0.8710372652845926\n",
      "STS.input.tweet-news.txt \t 0.7892451581165936\n",
      "STS/STS-data/STS2014-gold/ 0.7847565923191842 \n",
      "\n",
      "STS.input.answers-forums.txt \t 0.7191337358397288\n",
      "STS.input.answers-students.txt \t 0.7352044560438865\n",
      "STS.input.belief.txt \t 0.7918937332763348\n",
      "STS.input.headlines.txt \t 0.8308740059648289\n",
      "STS.input.images.txt \t 0.8748752641939654\n",
      "STS/STS-data/STS2015-gold/ 0.7903962390637489 \n",
      "\n",
      "SICK.input.txt \t 0.7355085347801373\n",
      "STS/SICK-data/ 0.7355085347801373 \n",
      "\n",
      "benchmark.input.txt \t 0.7953391927601844\n",
      "STSBenchmark/ 0.7953391927601844 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_STS(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
